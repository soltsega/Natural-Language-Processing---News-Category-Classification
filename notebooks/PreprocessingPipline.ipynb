{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Preprocessing Pipeline\n",
                "\n",
                "In this notebook, we will implement a text preprocessing pipeline to clean and normalize the AG News dataset. This includes:\n",
                "1. **HTML Removal**: Cleaning any leftover HTML tags.\n",
                "2. **Noise Reduction**: Removing press agency markers (e.g., Reuters, AP).\n",
                "3. **Normalization**: Lowercasing and removing special characters.\n",
                "4. **Lemmatization**: Reducing words to their base form."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import sys\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "# Add src to path to use data_loader\n",
                "sys.path.append(os.path.abspath(\"../\"))\n",
                "from src.data_loader import load_data, get_class_labels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize NLTK Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('omw-1.4')\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df, test_df = load_data()\n",
                "print(f\"Train shape: {train_df.shape}\")\n",
                "print(f\"Test shape: {test_df.shape}\")\n",
                "\n",
                "train_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Implement `clean_text` Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # 2. Remove HTML tags\n",
                "    text = re.sub(r'<.*?>', '', text)\n",
                "    \n",
                "    # 3. Remove Agency Tags (e.g., \"reuters - \", \"(ap) \")\n",
                "    # Pattern: agency name followed by \"-\" or inside parentheses at the start or end of snippet\n",
                "    text = re.sub(r'\\(\\w+\\)|\\w+\\s+-\\s+', '', text)\n",
                "    \n",
                "    # 4. Remove special characters and numbers\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                "    \n",
                "    # 5. Tokenization and Stopword Removal\n",
                "    words = text.split()\n",
                "    words = [w for w in words if w not in stop_words]\n",
                "    \n",
                "    # 6. Lemmatization\n",
                "    words = [lemmatizer.lemmatize(w) for w in words]\n",
                "    \n",
                "    return \" \".join(words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Apply Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Preprocessing training data...\")\n",
                "train_df['Clean_Description'] = train_df['Description'].apply(clean_text)\n",
                "\n",
                "print(\"Preprocessing test data...\")\n",
                "test_df['Clean_Description'] = test_df['Description'].apply(clean_text)\n",
                "\n",
                "train_df[['Description', 'Clean_Description']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processed_dir = os.path.join(\"..\", \"data\", \"processed\")\n",
                "os.makedirs(processed_dir, exist_ok=True)\n",
                "\n",
                "train_df.to_csv(os.path.join(processed_dir, \"clean_train.csv\"), index=False)\n",
                "test_df.to_csv(os.path.join(processed_dir, \"clean_test.csv\"), index=False)\n",
                "\n",
                "print(f\"Processed data saved to {processed_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
